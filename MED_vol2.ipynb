{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MED: Morphological Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import os.path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence#, masked_cross_entropy\n",
    "from masked_cross_entropy import *\n",
    "\n",
    "from med_dataset import MEDDataset, med_collate_fn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "PAD_TOKEN = 0\n",
    "START_TOKEN = 1\n",
    "END_TOKEN = 2\n",
    "UNK_TOKEN = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, n_layers=1, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embed_size, padding_idx=PAD_TOKEN)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        # Note: we run this all at once (over multiple batches of multiple sequences)\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:] # Sum bidirectional outputs\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "\n",
    "        # Create variable to store attention energies\n",
    "        attn_energies = Variable(torch.zeros(this_batch_size, max_len)) # B x S\n",
    "\n",
    "        if USE_CUDA:\n",
    "            attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # For each batch of encoder outputs\n",
    "        for b in range(this_batch_size):\n",
    "            # Calculate energy for each encoder output\n",
    "            for i in range(max_len):\n",
    "                attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1, resize to 1 x B x S\n",
    "        return F.softmax(attn_energies).unsqueeze(1)\n",
    "    \n",
    "    def score(self, hidden, encoder_output):\n",
    "        \n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.dot(encoder_output)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = hidden.dot(energy)\n",
    "            return energy\n",
    "        \n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = self.v.dot(energy)\n",
    "            return energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bahdanau et al. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, output_size, n_layers=1, dropout_p=0.1):\n",
    "        super(BahdanauAttnDecoderRNN, self).__init__()\n",
    "        \n",
    "        # Define parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, embed_size, padding_idx=PAD_TOKEN)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.attn = Attn('concat', hidden_size)\n",
    "        self.gru = nn.GRU(embed_size, hidden_size, n_layers, dropout=dropout_p)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, word_input, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "        # TODO: FIX BATCHING\n",
    "        \n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        word_embedded = self.embedding(word_input).view(1, 1, -1) # S=1 x B x N\n",
    "        word_embedded = self.dropout(word_embedded)\n",
    "        \n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attn(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x 1 x N\n",
    "        context = context.transpose(0, 1) # 1 x B x N\n",
    "        \n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat((word_embedded, context), 2)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        \n",
    "        # Final output layer\n",
    "        output = output.squeeze(0) # B x N\n",
    "        output = F.log_softmax(self.out(torch.cat((output, context), 1)))\n",
    "        \n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luong et al. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=PAD_TOKEN)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Choose attention model\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step at a time\n",
    "\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        batch_size = input_seq.size(0)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        embedded = embedded.view(1, batch_size, self.hidden_size) # S=1 x B x N\n",
    "\n",
    "        # Get current hidden state from input word and last hidden state\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        # Calculate attention from current RNN state and all encoder outputs;\n",
    "        # apply to encoder outputs to get weighted average\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x S=1 x N\n",
    "\n",
    "        # Attentional vector using the RNN hidden state and context vector\n",
    "        # concatenated together (Luong eq. 5)\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = F.tanh(self.concat(concat_input))\n",
    "\n",
    "        # Finally predict next token (Luong eq. 6, without softmax)\n",
    "        output = self.out(concat_output)\n",
    "\n",
    "        # Return final output, hidden state, and attention weights (for visualization)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(src_batch, src_lens, trg_batch, trg_lens, encoder, decoder, \n",
    "               encoder_optimizer, decoder_optimizer, criterion):\n",
    "    \n",
    "    # Zero gradients of both optimizers\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    #loss = 0 # Added onto for each word\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(src_batch, src_lens, None)\n",
    "    \n",
    "    # Prepare input and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([START_TOKEN] * batch_size))\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "\n",
    "    max_trg_len = max(trg_lens)\n",
    "    all_decoder_outputs = Variable(torch.zeros(max_trg_len, batch_size, decoder.output_size))\n",
    "\n",
    "    # Move new Variables to CUDA\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "\n",
    "    # Run through decoder one time step at a time\n",
    "    for t in range(max_trg_len):\n",
    "        decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        decoder_input = trg_batch[t] # Next input is current target\n",
    "\n",
    "    # Loss calculation and backpropagation\n",
    "    loss = masked_cross_entropy(\n",
    "        all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        trg_batch.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        trg_lens\n",
    "    )\n",
    "    loss.backward()\n",
    "    \n",
    "    # Clip gradient norms\n",
    "    enc_grads = torch.nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "    dec_grads = torch.nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "\n",
    "    # Update parameters with optimizers\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0]#, enc_grads, dec_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(encoder, decoder, checkpoint_dir):\n",
    "    enc_filename = \"{}/enc-{}.pth\".format(checkpoint_dir, time.strftime(\"%d%m%y-%H%M%S\"))\n",
    "    dec_filename = \"{}/dec-{}.pth\".format(checkpoint_dir, time.strftime(\"%d%m%y-%H%M%S\"))\n",
    "    #if not os.path.isfile(enc_filename):\n",
    "    #    open(enc_filename, 'w+')\n",
    "    #if not os.path.isfile(dec_filename):\n",
    "    #    open(dec_filename, 'w+')\n",
    "    torch.save(encoder.state_dict(), enc_filename)\n",
    "    torch.save(decoder.state_dict(), dec_filename)\n",
    "    print(\"Model saved.\")\n",
    "\n",
    "def train(dataset, batch_size, n_epochs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, \n",
    "          checkpoint_dir=None, save_every=500):\n",
    "    train_iter = DataLoader(dataset=dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=True,\n",
    "                            num_workers=4,\n",
    "                            collate_fn=med_collate_fn)\n",
    "    for i in range(n_epochs):\n",
    "        print(\"Epoch {}/{}\".format(i+1, n_epochs))\n",
    "        for batch_idx, batch in enumerate(train_iter):\n",
    "            input_batch, input_lengths, target_batch, target_lengths = batch\n",
    "            loss = train_step(input_batch, input_lengths, target_batch, target_lengths, \n",
    "                                 encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(\"batch: {}, loss: {}\".format(batch_idx, loss))\n",
    "            if checkpoint_dir:\n",
    "                if batch_idx % save_every == 0:\n",
    "                    save_checkpoint(encoder, decoder, checkpoint_dir)\n",
    "    \n",
    "    if checkpoint_dir:\n",
    "        save_checkpoint(encoder, decoder, checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring and Initializing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure models\n",
    "attn_model = 'dot'\n",
    "hidden_size = 100\n",
    "embed_size = 300\n",
    "n_layers = 1\n",
    "dropout = 0.1\n",
    "batch_size = 20\n",
    "checkpoint_dir = \"checkpoints\"\n",
    "\n",
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset\n",
    "dataset = MEDDataset(\"data/german-task2-train\")\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(len(dataset.in_vocab[0]), embed_size, hidden_size, n_layers, dropout=dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, hidden_size, len(dataset.out_vocab[0]), n_layers, dropout=dropout)\n",
    "\n",
    "# Initialize optimizers and criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Move models to GPU\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tome/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel/__main__.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/tome/faks/reinflection_projekt/Morphological-Encoder-Decoder/masked_cross_entropy.py:45: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  log_probs_flat = functional.log_softmax(logits_flat)\n",
      "/home/tome/faks/reinflection_projekt/Morphological-Encoder-Decoder/masked_cross_entropy.py:11: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.3. Note that arange generates values in [start; end), not [start; end].\n",
      "  seq_range = torch.range(0, max_len - 1).long()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0, loss: 2.3484816551208496\n",
      "Model saved.\n",
      "batch: 100, loss: 1.8132071495056152\n",
      "batch: 200, loss: 0.9923098683357239\n",
      "batch: 300, loss: 0.6644088625907898\n",
      "batch: 400, loss: 0.32585662603378296\n",
      "batch: 500, loss: 0.304109662771225\n",
      "Model saved.\n",
      "batch: 600, loss: 0.28833991289138794\n",
      "batch: 700, loss: 0.2485450953245163\n",
      "batch: 800, loss: 0.18855859339237213\n",
      "batch: 900, loss: 0.22373977303504944\n",
      "batch: 1000, loss: 0.1716632843017578\n",
      "Model saved.\n",
      "batch: 1100, loss: 0.32588452100753784\n",
      "batch: 1200, loss: 0.23767510056495667\n",
      "Epoch 2/20\n",
      "batch: 0, loss: 0.1861034333705902\n",
      "Model saved.\n",
      "batch: 100, loss: 0.17188267409801483\n",
      "batch: 200, loss: 0.24478620290756226\n",
      "batch: 300, loss: 0.3629983961582184\n",
      "batch: 400, loss: 0.1534886360168457\n",
      "batch: 500, loss: 0.2529307007789612\n",
      "Model saved.\n",
      "batch: 600, loss: 0.21300119161605835\n",
      "batch: 700, loss: 0.15426452457904816\n",
      "batch: 800, loss: 0.21426984667778015\n",
      "batch: 900, loss: 0.2350471466779709\n",
      "batch: 1000, loss: 0.14383719861507416\n",
      "Model saved.\n",
      "batch: 1100, loss: 0.18623031675815582\n",
      "batch: 1200, loss: 0.13975180685520172\n",
      "Epoch 3/20\n",
      "batch: 0, loss: 0.1524718552827835\n",
      "Model saved.\n",
      "batch: 100, loss: 0.12444739043712616\n",
      "batch: 200, loss: 0.14381854236125946\n",
      "batch: 300, loss: 0.08764967322349548\n",
      "batch: 400, loss: 0.19411525130271912\n",
      "batch: 500, loss: 0.18415772914886475\n",
      "Model saved.\n",
      "batch: 600, loss: 0.12515802681446075\n",
      "batch: 700, loss: 0.17951254546642303\n",
      "batch: 800, loss: 0.18018314242362976\n",
      "batch: 900, loss: 0.154537633061409\n",
      "batch: 1000, loss: 0.3447813093662262\n",
      "Model saved.\n",
      "batch: 1100, loss: 0.19887885451316833\n",
      "batch: 1200, loss: 0.19514554738998413\n",
      "Epoch 4/20\n",
      "batch: 0, loss: 0.12104721367359161\n",
      "Model saved.\n",
      "batch: 100, loss: 0.12042083591222763\n",
      "batch: 200, loss: 0.1547287553548813\n",
      "batch: 300, loss: 0.23146268725395203\n",
      "batch: 400, loss: 0.14444556832313538\n",
      "batch: 500, loss: 0.13671275973320007\n",
      "Model saved.\n",
      "batch: 600, loss: 0.12745335698127747\n",
      "batch: 700, loss: 0.16381163895130157\n",
      "batch: 800, loss: 0.11382703483104706\n",
      "batch: 900, loss: 0.18197683990001678\n",
      "batch: 1000, loss: 0.0973864197731018\n",
      "Model saved.\n",
      "batch: 1100, loss: 0.12288942188024521\n",
      "batch: 1200, loss: 0.12701548635959625\n",
      "Epoch 5/20\n",
      "batch: 0, loss: 0.11990915983915329\n",
      "Model saved.\n",
      "batch: 100, loss: 0.14553971588611603\n",
      "batch: 200, loss: 0.14309029281139374\n",
      "batch: 300, loss: 0.10140225291252136\n",
      "batch: 400, loss: 0.16702304780483246\n",
      "batch: 500, loss: 0.1489148885011673\n",
      "Model saved.\n",
      "batch: 600, loss: 0.1388903260231018\n",
      "batch: 700, loss: 0.10077271610498428\n",
      "batch: 800, loss: 0.11594170331954956\n",
      "batch: 900, loss: 0.0941162183880806\n",
      "batch: 1000, loss: 0.14415264129638672\n",
      "Model saved.\n",
      "batch: 1100, loss: 0.17182521522045135\n",
      "batch: 1200, loss: 0.09296059608459473\n",
      "Epoch 6/20\n",
      "batch: 0, loss: 0.12751375138759613\n",
      "Model saved.\n",
      "batch: 100, loss: 0.2073395699262619\n",
      "batch: 200, loss: 0.08597439527511597\n",
      "batch: 300, loss: 0.14499783515930176\n",
      "batch: 400, loss: 0.1347658634185791\n",
      "batch: 500, loss: 0.11269279569387436\n",
      "Model saved.\n",
      "batch: 600, loss: 0.08798671513795853\n",
      "batch: 700, loss: 0.11854240298271179\n",
      "batch: 800, loss: 0.11027917265892029\n",
      "batch: 900, loss: 0.09191137552261353\n",
      "batch: 1000, loss: 0.11225372552871704\n",
      "Model saved.\n",
      "batch: 1100, loss: 0.10178005695343018\n",
      "batch: 1200, loss: 0.10957979410886765\n",
      "Epoch 7/20\n",
      "batch: 0, loss: 0.14738953113555908\n",
      "Model saved.\n",
      "batch: 100, loss: 0.10657443106174469\n",
      "batch: 200, loss: 0.07201769948005676\n",
      "batch: 300, loss: 0.09051477164030075\n",
      "batch: 400, loss: 0.14344224333763123\n",
      "batch: 500, loss: 0.1161470040678978\n",
      "Model saved.\n",
      "batch: 600, loss: 0.05948622524738312\n",
      "batch: 700, loss: 0.10640068352222443\n",
      "batch: 800, loss: 0.10819197446107864\n",
      "batch: 900, loss: 0.10683497041463852\n",
      "batch: 1000, loss: 0.07164141535758972\n",
      "Model saved.\n",
      "batch: 1100, loss: 0.0976814404129982\n",
      "batch: 1200, loss: 0.08777378499507904\n",
      "Epoch 8/20\n",
      "batch: 0, loss: 0.08454570919275284\n",
      "Model saved.\n",
      "batch: 100, loss: 0.1133340522646904\n",
      "batch: 200, loss: 0.08843258768320084\n",
      "batch: 300, loss: 0.1218290701508522\n",
      "batch: 400, loss: 0.11136135458946228\n",
      "batch: 500, loss: 0.11837886273860931\n",
      "Model saved.\n",
      "batch: 600, loss: 0.10634476691484451\n",
      "batch: 700, loss: 0.10806774348020554\n",
      "batch: 800, loss: 0.14993645250797272\n",
      "batch: 900, loss: 0.17810173332691193\n",
      "batch: 1000, loss: 0.050901368260383606\n",
      "Model saved.\n",
      "batch: 1100, loss: 0.12530261278152466\n",
      "batch: 1200, loss: 0.06835976243019104\n",
      "Epoch 9/20\n",
      "batch: 0, loss: 0.07386079430580139\n",
      "Model saved.\n",
      "batch: 100, loss: 0.15196652710437775\n",
      "batch: 200, loss: 0.074615478515625\n",
      "batch: 300, loss: 0.08320163935422897\n",
      "batch: 400, loss: 0.09205994755029678\n",
      "batch: 500, loss: 0.059338536113500595\n",
      "Model saved.\n",
      "batch: 600, loss: 0.08121734857559204\n",
      "batch: 700, loss: 0.08216707408428192\n",
      "batch: 800, loss: 0.09885763376951218\n",
      "batch: 900, loss: 0.09551426768302917\n",
      "batch: 1000, loss: 0.06909115612506866\n",
      "Model saved.\n",
      "batch: 1100, loss: 0.1510302871465683\n",
      "batch: 1200, loss: 0.08419658243656158\n",
      "Epoch 10/20\n",
      "batch: 0, loss: 0.0665738582611084\n",
      "Model saved.\n",
      "batch: 100, loss: 0.1220228374004364\n",
      "batch: 200, loss: 0.07525312155485153\n",
      "batch: 300, loss: 0.095022052526474\n",
      "batch: 400, loss: 0.10550730675458908\n",
      "batch: 500, loss: 0.06725016981363297\n",
      "Model saved.\n",
      "batch: 600, loss: 0.08772122114896774\n",
      "batch: 700, loss: 0.11775873601436615\n",
      "batch: 800, loss: 0.0897708609700203\n",
      "batch: 900, loss: 0.13874635100364685\n",
      "batch: 1000, loss: 0.13742728531360626\n",
      "Model saved.\n",
      "batch: 1100, loss: 0.10014592111110687\n",
      "batch: 1200, loss: 0.08035553246736526\n",
      "Epoch 11/20\n",
      "batch: 0, loss: 0.08829107135534286\n",
      "Model saved.\n",
      "batch: 100, loss: 0.1384042203426361\n",
      "batch: 200, loss: 0.11991578340530396\n",
      "batch: 300, loss: 0.0870896503329277\n",
      "batch: 400, loss: 0.04132005199790001\n",
      "batch: 500, loss: 0.07249314337968826\n",
      "Model saved.\n",
      "batch: 600, loss: 0.08289555460214615\n",
      "batch: 700, loss: 0.08715373277664185\n",
      "batch: 800, loss: 0.11157157272100449\n",
      "batch: 900, loss: 0.1519124060869217\n",
      "batch: 1000, loss: 0.06474071741104126\n",
      "Model saved.\n",
      "batch: 1100, loss: 0.06376241147518158\n",
      "batch: 1200, loss: 0.0664939433336258\n",
      "Epoch 12/20\n",
      "batch: 0, loss: 0.08128338307142258\n",
      "Model saved.\n",
      "batch: 100, loss: 0.08088715374469757\n",
      "batch: 200, loss: 0.07367481291294098\n",
      "batch: 300, loss: 0.06769301742315292\n",
      "batch: 400, loss: 0.0489727184176445\n",
      "batch: 500, loss: 0.11734724044799805\n",
      "Model saved.\n",
      "batch: 600, loss: 0.07704679667949677\n",
      "batch: 700, loss: 0.10138745605945587\n",
      "batch: 800, loss: 0.09468582272529602\n",
      "batch: 900, loss: 0.09853096306324005\n",
      "batch: 1000, loss: 0.05837945640087128\n",
      "Model saved.\n",
      "batch: 1100, loss: 0.08956733345985413\n",
      "batch: 1200, loss: 0.061101034283638\n",
      "Epoch 13/20\n",
      "batch: 0, loss: 0.05899355933070183\n",
      "Model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-55:\n",
      "Process Process-54:\n",
      "Process Process-56:\n",
      "Process Process-53:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/multiprocessing/synchronize.py\", line 96, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method DataLoaderIter.__del__ of <torch.utils.data.dataloader.DataLoaderIter object at 0x7f2d396876d8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 331, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/home/tome/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 175, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 28886) exited unexpectedly with exit code 1.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-bb5e99c7ff5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mdecoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       checkpoint_dir)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-85e079817131>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataset, batch_size, n_epochs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, checkpoint_dir, save_every)\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0minput_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             loss = train_step(input_batch, input_lengths, target_batch, target_lengths, \n\u001b[0;32m---> 24\u001b[0;31m                                  encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch: {}, loss: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-ebab3b5f4ef2>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(src_batch, src_lens, trg_batch, trg_lens, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mtrg_lens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     )\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Clip gradient norms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#encoder.load_state_dict(torch.load(\"checkpoints/enc-150518-224957.pth\"))\n",
    "#decoder.load_state_dict(torch.load(\"checkpoints/dec-150518-224957.pth\"))\n",
    "\n",
    "train(dataset, \n",
    "      batch_size, \n",
    "      n_epochs, \n",
    "      encoder, \n",
    "      decoder, \n",
    "      encoder_optimizer, \n",
    "      decoder_optimizer, \n",
    "      criterion, \n",
    "      checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_seq, input_len, encoder, decoder, max_length=40):\n",
    "    #input_lengths = [len(input_seq)]\n",
    "    #input_seqs = [indexes_from_sentence(input_lang, input_seq)]\n",
    "    #input_batches = Variable(torch.LongTensor(input_seqs), volatile=True).transpose(0, 1)\n",
    "    input_seq = Variable(input_seq, volatile=True)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        input_seq = input_seq.cuda()\n",
    "        \n",
    "    # Set to not-training mode to disable dropout\n",
    "    encoder.train(False)\n",
    "    decoder.train(False)\n",
    "    \n",
    "    # Run through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_seq, input_len, None)\n",
    "\n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([START_TOKEN]), volatile=True) # SOS\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers] # Use last (forward) hidden state from encoder\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Store output words and attention states\n",
    "    decoded_chars = []\n",
    "    decoder_attentions = torch.zeros(max_length + 1, max_length + 1)\n",
    "    \n",
    "    # Run through decoder\n",
    "    for t in range(max_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "        #decoder_attentions[t,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "\n",
    "        # Choose top word from output\n",
    "        prob, token_idx = decoder_output.data.topk(1)\n",
    "        tok = token_idx[0][0]\n",
    "        if tok == END_TOKEN:\n",
    "            break\n",
    "        else:\n",
    "            decoded_chars.append(dataset.out_vocab[0][tok])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([tok]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Set back to training mode\n",
    "    encoder.train(True)\n",
    "    decoder.train(True)\n",
    "    \n",
    "    return \"\".join(decoded_chars), decoder_attentions[:t+1, :len(encoder_outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_dataset(file_name, encoder, decoder):\n",
    "    dataset = MEDDataset(file_name, train=False)\n",
    "    test_iter = DataLoader(dataset=dataset,\n",
    "                           batch_size=1,\n",
    "                            shuffle=False,\n",
    "                            num_workers=4,\n",
    "                            collate_fn=med_collate_fn)\n",
    "    \n",
    "    decoded_words = []\n",
    "    for input_seq, input_len, _, _ in test_iter:\n",
    "        decoded_word, attentions = translate(input_seq, input_len, encoder, decoder)\n",
    "        decoded_words.append(decoded_word)\n",
    "        \n",
    "    with open(file_name + \"-results\", 'w') as outfile:\n",
    "        outfile.write(\"\\n\".join(decoded_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tome/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel/__main__.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "translate_dataset(\"data/german-task2-test\", encoder, decoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
